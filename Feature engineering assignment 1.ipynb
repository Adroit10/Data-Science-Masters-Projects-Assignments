{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7fb578b",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e7dd7",
   "metadata": {},
   "source": [
    "Missing data or missing values occur when no data value is stored for the variable in an observation. Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data. Incomplete data can bias the results of the machine learning models and/or reduce the accuracy of the model. \n",
    "\n",
    "k-NN, Naive Bayes and Random Forest algorithms can support missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af55795",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f727de",
   "metadata": {},
   "source": [
    "There are many techniques that can be used to handle missing values in our dataset these may include the following ----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b6cb9",
   "metadata": {},
   "source": [
    "###### 1. dropping of columns or rows having the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b09005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c8f6fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived         0\n",
       "pclass           0\n",
       "sex              0\n",
       "age            177\n",
       "sibsp            0\n",
       "parch            0\n",
       "fare             0\n",
       "embarked         2\n",
       "class            0\n",
       "who              0\n",
       "adult_male       0\n",
       "deck           688\n",
       "embark_town      2\n",
       "alive            0\n",
       "alone            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()  # here we can see that our dataset has missing values in columns -[age,embarked,deck,embark_town]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b342b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape  # here we have 891 entries in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faeee20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## removing the rows that have null values\n",
    "df.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e59c51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(axis=1).shape  # removing the columns having missing values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75873877",
   "metadata": {},
   "source": [
    "Here we observe that originally the dataset was containing 891 rows of data entries but after removing the rows with missing values we are left with only 182 rows and if we remove the columns conntaining the same we are left with 11 columns. There is a huge loss of data so we conclude that this method is not an effective way of removing missing values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1b10ed",
   "metadata": {},
   "source": [
    "###### 2. Mean Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc0569",
   "metadata": {},
   "source": [
    "In mean imputation we replace the missing values in the dataset by the mean value of the column where the missing value is present. Mean imputation works for data which is normally distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08d1dd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "886    27.0\n",
       "887    19.0\n",
       "888     NaN\n",
       "889    26.0\n",
       "890    32.0\n",
       "Name: age, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76a0aa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mean_age']=df['age'].fillna(df['age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a96d97ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "886    27.000000\n",
       "887    19.000000\n",
       "888    29.699118\n",
       "889    26.000000\n",
       "890    32.000000\n",
       "Name: mean_age, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['mean_age'].tail()  # the 888th entry is replace by the mean value of the age column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc16ed3",
   "metadata": {},
   "source": [
    "###### 3. Median Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df194ef",
   "metadata": {},
   "source": [
    "In median imputation we replace the missing values by the median value of the column. Median imputation is used for data that is skewed in nature and may have otliers present in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "564cb910",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['median_age']=df['age'].fillna(df['age'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12bf4833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "886    27.0\n",
       "887    19.0\n",
       "888    28.0\n",
       "889    26.0\n",
       "890    32.0\n",
       "Name: median_age, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['median_age'].tail() ### 888th entry replaced by the median value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c01dde",
   "metadata": {},
   "source": [
    "###### 4. Mode Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188e029f",
   "metadata": {},
   "source": [
    "Mode imputation is a technique that is used for filling the missing values in categorical variables. This fills the missing value with the mode or the most occuring sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb05ca93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S', 'C', 'Q', nan], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['embarked'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b182dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "      <th>mean_age</th>\n",
       "      <th>median_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "      <td>62.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass     sex   age  sibsp  parch  fare embarked  class  \\\n",
       "61          1       1  female  38.0      0      0  80.0      NaN  First   \n",
       "829         1       1  female  62.0      0      0  80.0      NaN  First   \n",
       "\n",
       "       who  adult_male deck embark_town alive  alone  mean_age  median_age  \n",
       "61   woman       False    B         NaN   yes   True      38.0        38.0  \n",
       "829  woman       False    B         NaN   yes   True      62.0        62.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['embarked'].isnull()]  # there are two entries with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d30a4eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_val=df[df['embarked'].notna()]['embarked'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2abe0025",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_embarked']=df['embarked'].fillna(mode_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d41924b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['new_embarked'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8f97d8",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b5dd2",
   "metadata": {},
   "source": [
    "Imbalanced data refers to those types of datasets where the target class has an uneven distribution of observations, i.e one class label has a very high number of observations and the other has a very low number of observations. These classes can be classified as minority and majority classes.\n",
    "Sometimes when the records of a certain class are much more than the other class, our classifier may get biased towards the prediction of that particular class and effects the overall performance of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138efa7f",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9011368",
   "metadata": {},
   "source": [
    "Upsampling is a procedure where synthetically generated data points (corresponding to minority class) are injected into the dataset. After this process, the counts of both labels are almost the same. This equalization procedure prevents the model from inclining towards the majority class. Furthermore, the interaction between the target classes remains unaltered. And also, the upsampling mechanism introduces bias into the system because of the additional information.\n",
    "\n",
    "Downsampling is a mechanism that reduces the count of training samples falling under the majority class. As it helps to even up the counts of target categories. By removing the collected data, we tend to lose so much valuable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a35e3143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "class_ratio=0.8\n",
    "number_samples=1200\n",
    "n_class_0=int(number_samples*class_ratio)\n",
    "n_class_1=number_samples-n_class_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fb302d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0=pd.DataFrame({\n",
    "    'f1':np.random.normal(loc=0,scale=1,size=n_class_0),\n",
    "    'f2':np.random.normal(loc=0,scale=1,size=n_class_0),\n",
    "    'target':[0]*n_class_0\n",
    "})\n",
    "class_1=pd.DataFrame({\n",
    "    'f1':np.random.normal(loc=2,scale=1,size=n_class_1),\n",
    "    'f2':np.random.normal(loc=2,scale=1,size=n_class_1),\n",
    "    'target':[1]*n_class_1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11cb59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([class_0,class_1]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d9e5d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.496714</td>\n",
       "      <td>0.642723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.138264</td>\n",
       "      <td>1.329153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.647689</td>\n",
       "      <td>0.196521</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.523030</td>\n",
       "      <td>0.709004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.234153</td>\n",
       "      <td>-0.089736</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>1.383639</td>\n",
       "      <td>0.550355</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>1.624804</td>\n",
       "      <td>1.078140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>1.682285</td>\n",
       "      <td>0.996043</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>3.281644</td>\n",
       "      <td>2.207267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>2.557691</td>\n",
       "      <td>2.069344</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            f1        f2  target\n",
       "0     0.496714  0.642723       0\n",
       "1    -0.138264  1.329153       0\n",
       "2     0.647689  0.196521       0\n",
       "3     1.523030  0.709004       0\n",
       "4    -0.234153 -0.089736       0\n",
       "...        ...       ...     ...\n",
       "1195  1.383639  0.550355       1\n",
       "1196  1.624804  1.078140       1\n",
       "1197  1.682285  0.996043       1\n",
       "1198  3.281644  2.207267       1\n",
       "1199  2.557691  2.069344       1\n",
       "\n",
       "[1200 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc074669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    960\n",
       "1    240\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()  # we have created an imbalanced dataset with 0 being the majority class and 1 being the minority class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72af53c5",
   "metadata": {},
   "source": [
    "## Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36ef38ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsampling\n",
    "df_minority=df[df['target']==1]\n",
    "df_majority=df[df['target']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a28e8cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "df_majority_upsampled=resample(df_minority,replace=True,n_samples=len(df_majority),random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6edd51a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>3.360659</td>\n",
       "      <td>3.235782</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>2.817890</td>\n",
       "      <td>1.339679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>1.230858</td>\n",
       "      <td>0.548824</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>4.644343</td>\n",
       "      <td>1.164857</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>3.800511</td>\n",
       "      <td>0.221412</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>2.670481</td>\n",
       "      <td>2.500666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>0.770450</td>\n",
       "      <td>2.987335</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>0.106385</td>\n",
       "      <td>3.617213</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>1.630390</td>\n",
       "      <td>3.533434</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>1.373283</td>\n",
       "      <td>2.744192</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>960 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            f1        f2  target\n",
       "1062  3.360659  3.235782       1\n",
       "1139  2.817890  1.339679       1\n",
       "1052  1.230858  0.548824       1\n",
       "974   4.644343  1.164857       1\n",
       "1066  3.800511  0.221412       1\n",
       "...        ...       ...     ...\n",
       "1136  2.670481  2.500666       1\n",
       "970   0.770450  2.987335       1\n",
       "1044  0.106385  3.617213       1\n",
       "985   1.630390  3.533434       1\n",
       "1022  1.373283  2.744192       1\n",
       "\n",
       "[960 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_majority_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6413e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upsampled=pd.concat([df_majority,df_majority_upsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "113103a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    960\n",
       "1    960\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_upsampled['target'].value_counts() # the mibority class have been upsampled and have the same number of occurance as the majority class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b787d06d",
   "metadata": {},
   "source": [
    "## Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84d6e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "class_ratio=0.8\n",
    "number_samples=1200\n",
    "n_class_0=int(number_samples*class_ratio)\n",
    "n_class_1=number_samples-n_class_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4d14011",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0=pd.DataFrame({\n",
    "    'f1':np.random.normal(loc=0,scale=1,size=n_class_0),\n",
    "    'f2':np.random.normal(loc=0,scale=1,size=n_class_0),\n",
    "    'target':[0]*n_class_0\n",
    "})\n",
    "class_1=pd.DataFrame({\n",
    "    'f1':np.random.normal(loc=2,scale=1,size=n_class_1),\n",
    "    'f2':np.random.normal(loc=2,scale=1,size=n_class_1),\n",
    "    'target':[1]*n_class_1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0d2c603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    960\n",
       "1    240\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.concat([class_0,class_1]).reset_index(drop=True)\n",
    "df['target'].value_counts()  # we have created an imbalanced dataset with 0 being the majority class and 1 being the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01379693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minority=df[df['target']==1]\n",
    "df_majority=df[df['target']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4b75276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority_downsampled=resample(df_majority,n_samples=len(df_minority),random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02bc5460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-0.342715</td>\n",
       "      <td>1.148766</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>0.074095</td>\n",
       "      <td>0.184680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0.202923</td>\n",
       "      <td>-0.131257</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>1.441273</td>\n",
       "      <td>-0.557492</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.886186</td>\n",
       "      <td>-1.294681</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0.872321</td>\n",
       "      <td>-0.613403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>1.586017</td>\n",
       "      <td>-0.581681</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>0.456753</td>\n",
       "      <td>-0.620848</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>-0.158008</td>\n",
       "      <td>0.436560</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>-0.125787</td>\n",
       "      <td>-0.367028</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           f1        f2  target\n",
       "102 -0.342715  1.148766       0\n",
       "435  0.074095  0.184680       0\n",
       "860  0.202923 -0.131257       0\n",
       "270  1.441273 -0.557492       0\n",
       "106  1.886186 -1.294681       0\n",
       "..        ...       ...     ...\n",
       "376  0.872321 -0.613403       0\n",
       "282  1.586017 -0.581681       0\n",
       "957  0.456753 -0.620848       0\n",
       "632 -0.158008  0.436560       0\n",
       "627 -0.125787 -0.367028       0\n",
       "\n",
       "[240 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_majority_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f93f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_downsampled=pd.concat([df_minority,df_majority_downsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5d3cb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    240\n",
       "0    240\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_downsampled['target'].value_counts()  # the majority class has been downsampled to the same number of occurance of the minority class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a6de82",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a384650c",
   "metadata": {},
   "source": [
    "Data augmentation is a technique of artificially increasing the training set by creating modified copies of a dataset using existing data. It includes making minor changes to the dataset or using deep learning to generate new data points.Augmented data is driven from original data with some minor changes. In the case of image augmentation, we make geometric and color space transformations (flipping, resizing, cropping, brightness, contrast) to increase the size and diversity of the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfd0582",
   "metadata": {},
   "source": [
    "SMOTE(SyntheticMinorityOversamplingTechnique) works based on the KNearestNeighbours algorithm, synthetically generating data points that fall in the proximity of the already existing outnumbered group. The input records should not contain any null values when applying this approach. In other words it fills the link between two data points with synthetically generated datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598fc864",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b456861f",
   "metadata": {},
   "source": [
    "In simple terms, an outlier also called aberrations, abnormal points, anomalies, etc. is an extremely high or extremely low data point relative to the nearest data point and the rest of the neighboring co-existing values in a data graph or dataset you're working with. Outliers are extreme values that stand out greatly from the overall pattern of values in a dataset or graph.\n",
    "\n",
    "It is essential to detect and handle outliers in a dataset as it can have a significant impact on many statistical methods, such as mean, variance, etc., and the performance of the ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f7892a",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3324401",
   "metadata": {},
   "source": [
    "While working on the customer dataset we'll first observe the type of values present in different columns of our dataset. If there are are numerical values present in the dataset we'll observe the distribution of dataset if the distribution is normally distributed and there are no outliers present in the dataset we can use mean imputation technique to fill the missing values in the dataset else if the data is skewed or have outliers present we use median imputation beacause mean value gets affected by the presence of outliers in the dataset.\n",
    "\n",
    "For categorical variables we use mode imputation in which the missing value is replaced by the mode or the highest uccuring sequence in the dataset column.\n",
    "\n",
    "We can also use deletion techniques such as column or row deletion containing the missing values but this leads to loss of data.\n",
    "\n",
    "Another advance imputation technique is the K_Nearest Neighbour Imputation in which we make use of machine learning algorithm to impute the data (using Euclidian distance metric)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad57060f",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56386d1a",
   "metadata": {},
   "source": [
    "When working with a big dataset that has a small portion/percentage of missing data we can detect the missing values visually using Missingno library that presents a series of visualizations to recognize the behaviour and distribution of missing data inside a pandas data frame.  It can be in the form of a barplot, matrix plot, heatmap, or a dendrogram.\n",
    "The matrix functionality of the missingno library gives us a visual representation of the data frame with white lines depicting the position of the missing values in the column, we can also use the heatmap functionality of the library to give us a more clear understanding of the relationship between the missing values of two features. Observing these patterns we can determine the relationship that the missing values might have with other features missing values, to determine wheather the data is missing at random(MAR) , missing completely at random(MCAR) or missing not at random(MNAR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45bc06",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64718ff",
   "metadata": {},
   "source": [
    "Suppose we are working on a medical diagnosis project there will be an imbalance of data because there are only small number of patients that have the condition of interest this could lead to us getting a pretty high accuracy just by predicting the majority class, but we fail to capture the minority class, which is most often the point of creating the model in the first place.In a dataset with highly unbalanced classes, the classifier will always predict the most common class without performing any analysis of the features, and it will have a high accuracy rate, obviously not the correct one.\n",
    "\n",
    " Therefore resampling techniques are used to solve class Imbalance problem.It consists of removing samples from the majority class (under-sampling) and/or adding more examples from the minority class (over-sampling). We can use techniques such as --  Random Under-Sampling With Imblearn, Random Over-Sampling With imblearn, Synthetic Minority Oversampling Technique (SMOTE), and we can change the performance metrics and use Confusion Matrix, Precision, Recall, F1 score, Area under ROC curve\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c8bc4",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecda900",
   "metadata": {},
   "source": [
    "When attempting to estimate customer satisfaction our dataset can be unbalanced to solve this problem we can use resampling techniques to make our dataset more balanced andso that our model is not biased towwards the majority class. Resampling can include techniques such as under-sampling or down sampling in which we reduce the majority data points and bring them close or equal to the minority class but  this technique may lead to loss of data, we could also upsample our minority class to bring it close to the majority class.\n",
    "The resample functionality is imported from the sklearn.utils liabrary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e7ad9e",
   "metadata": {},
   "source": [
    "## Question 11\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a5836",
   "metadata": {},
   "source": [
    "For the estimation of occurance of a rare event we have an unbalanced data as there are lesser records for the occurance of the event so to make the unbalanced dataset balanced we can upsample the minority class of data containing the information of the existance of the rare event. The resample functionality of the sklearn.utils liabrary can be used to upsample the data to bring it closer to the majority class data. We can also use the imblearn(imbalance learn liabrary ) to import SMOTE(Sysnthetic Minority Over-sampling Technique) which handles the imbalance of the data by crreating synthetic datapoints between two minority datapoints using the kNN algorithm.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabcdab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
