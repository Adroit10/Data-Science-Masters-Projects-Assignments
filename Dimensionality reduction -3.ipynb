{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9aecb9c",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e28c8e",
   "metadata": {},
   "source": [
    "Eigenvalues and eigenvectors are concepts from linear algebra that play a crucial role in various mathematical and computational applications, including the Eigen-Decomposition approach.\n",
    "\n",
    "An eigenvalue of a square matrix is a scalar (λ) that represents how the matrix transforms its corresponding eigenvector. In the context of a matrix A, an eigenvalue λ and its corresponding eigenvector v satisfy the equation:\n",
    "\n",
    "Av=λv\n",
    "\n",
    "In other words, when the matrix A is multiplied by its eigenvector v, the result is a scaled version of the same eigenvector, where the scaling factor is the eigenvalue λ.\n",
    "\n",
    "An eigenvector is a non-zero vector that remains in the same direction after being multiplied by a matrix. In the context of a matrix A and an eigenvalue λ, the vector v is an eigenvector if:\n",
    "\n",
    "Av=λv\n",
    "\n",
    "Eigenvectors associated with distinct eigenvalues are linearly independent.\n",
    "\n",
    "Eigen-Decomposition is an approach to decompose a square matrix A into a product of its eigenvectors and eigenvalues. For a matrix A with linearly independent eigenvectors v1,v2,…,vn and corresponding eigenvalues λ1,λ2,…,λn the eigen-decomposition is given by:\n",
    "\n",
    "A=VΛV^−1\n",
    "\n",
    "V is a matrix whose columns are the eigenvectors of A.\n",
    "Λ is a diagonal matrix with the eigenvalues of A on the diagonal.\n",
    "V^ −1 is the inverse of the matrix V."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c288ff00",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7034df",
   "metadata": {},
   "source": [
    "Eigen decomposition is a fundamental concept in linear algebra. It involves decomposing a square matrix into a set of eigenvectors and eigenvalues. For a given square matrix A  eigendecomposition expresses A as the product of three matrices:\n",
    "\n",
    "A=VΛV^−1\n",
    "\n",
    "V is a matrix whose columns are the eigenvectors of A. Λ is a diagonal matrix with the eigenvalues of A on the diagonal. V^ −1 is the inverse of the matrix V.\n",
    "Eigendecomposition is closely tied to eigenvalues and eigenvectors. The eigenvalues (λi) represent the scaling factors by which the corresponding eigenvectors (vi) are stretched or compressed when the matrix A is applied to them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b9f976",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53419908",
   "metadata": {},
   "source": [
    "For a square matrix A to be diagonalizable using the Eigen-Decomposition approach, it must satisfy certain conditions. The key conditions are as follows:\n",
    "\n",
    "1. A must have a full set of linearly independent eigenvectors. Specifically, the matrix A must have n linearly independent eigenvectors, where n is the size of the matrix.\n",
    "\n",
    "2. The eigenvectors of A must form a complete set, meaning that they span the entire vector space. In other words, the set of eigenvectors must be able to express any vector in the space.\n",
    "\n",
    "\n",
    "Proof:\n",
    "\n",
    "Suppose we have a square matrix A of size n X n and let λ1,λ2,...,λk be its distinct eigenvalues with corresponding linearly independent eigenvectors v1,v2,v3,...,vk \n",
    "\n",
    "Assume that A is diagonalizable. This implies that we can write\n",
    "A = PDP^-1 where P is the matrix whose columns are the eigen vetors v1,v2,...,vk and D is the diagonal matrix with the corresponding eigenvalues λ1,λ2,...,λk on the diagonal.\n",
    "\n",
    "Since P is formed by stacking linearly independent eigenvectors, P is invertible. The inverse P ^−1 exists. Now let'sconsider the product AP = PDP ^-1 P = PD. Since P contains only linearly independant eigen vectors the columns of PD are linear combinations of the eigenvectors v1,v2,...,vk. Thus, the columns of AP are linear combinations of the eigen vectors, and AP can be expressed as a linear combination of vi's.\n",
    "\n",
    "Since the columns of AP are linear combinations of the eigen vectors, AP spans the same subspace as the eigen vectors. Since AP is also n X n, it spans the entire n-dimensional vector space.\n",
    "\n",
    "therefore, the eigen vectors v1,v2,...,vk form a complete set.\n",
    "\n",
    "In conclusion, if A is diagonalizable, it must have a full set of linearly independent eigen vectors, and these eigen vectors must form a complete set spanning the entire vector space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e66ccec",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002aa516",
   "metadata": {},
   "source": [
    "The Spectral Theorem is a fundamental result in linear algebra that provides a significant link between symmetric matrices, diagonalizability, and the Eigen-Decomposition approach. The Spectral Theorem states that every symmetric matrix is diagonalizable and can be decomposed into a set of orthogonal eigen vectors and corresponding real eigen values. This theorem is crucial for understanding the properties of symmetric matrices and plays a key role in the Eigen-Decomposition approach.\n",
    "\n",
    "\n",
    "The Spectral Theorem essentially states that for any symmetric matrix A, there exists an orthogonal matrix P such that A=PDP ^T, where D is a diagonal matrix containing the real eigenvalues of A. This directly establishes the diagonalizability of symmetric matrices using an orthogonal matrix.\n",
    "\n",
    "Let's consider a symmetric matrix A = [3 -2 , -2 2]\n",
    "\n",
    "det(A-λI) = 0\n",
    "\n",
    "results to two eigenvalues λ1= 4.56 and λ2= 0.44\n",
    "\n",
    "and the corresponding eigenvectors are :\n",
    "\n",
    "v1 = [0.82 -0.57]\n",
    "\n",
    "v2 = [0.57 0.82]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173e786b",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411b6081",
   "metadata": {},
   "source": [
    "Finding the eigenvalues of a matrix involves solving a characteristic equation associated with the matrix. Here are the steps to find the eigenvalues of a square matrix A :\n",
    "\n",
    "For a matrix A, the characteristic equation is given by det(A−λI)=0, where I is the identity matrix and λ is the eigenvalue. Solve the characteristic equation to find the eigenvalues λ. This often involves solving a polynomial equation.\n",
    "The eigenvalues λ represent the scalar values by which certain vectors (eigenvectors) are stretched or compressed when the matrix A is applied to them. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f69ecce",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4814f052",
   "metadata": {},
   "source": [
    "An eigenvector is a non-zero vector that remains in the same direction after being multiplied by a matrix. In the context of a matrix A and an eigenvalue λ, the vector v is an eigenvector if:\n",
    "\n",
    "Av=λv\n",
    "\n",
    "Eigenvectors associated with distinct eigenvalues are linearly independent.\n",
    "\n",
    "Mathematically, for an eigenvector v, the eigenvalue equation is Av=λv. The eigenvalues are crucial in understanding the behavior of linear transformations represented by matrices.\n",
    "An eigenvector v is a non-zero vector that remains in the same direction (up to a scaling factor) after a linear transformation represented by the matrix A. Mathematically, Av=λv, where λ is the eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f20234",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803c8f5c",
   "metadata": {},
   "source": [
    "he geometric interpretation of eigenvectors and eigenvalues provides insights into the transformational behavior of a matrix.\n",
    "\n",
    "An eigenvector of a matrix represents a direction in space that remains unchanged (up to scaling) after the matrix transformation. When a matrix is applied to an eigenvector, the eigenvector is scaled by its corresponding eigenvalue. If the eigenvalue is greater than 1, the eigenvector undergoes stretching along its direction. If the eigenvalue is between 0 and 1, the eigenvector undergoes compression. If the eigenvalue is negative, the eigenvector undergoes reflection.\n",
    "\n",
    "Eigenvalues represent the scaling factors by which the corresponding eigenvectors are stretched, compressed, or reflected during the matrix transformation.\n",
    "\n",
    "Diagonalization of a matrix involves expressing it as a product of matrices involving its eigenvectors and eigenvalues. In this diagonal form, the matrix operates independently along each eigenvector direction.\n",
    "In PCA, eigenvectors of the covariance matrix represent the principal components of the data. These directions capture the maximum variance, and the corresponding eigenvalues indicate the amount of variance along each principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48759b69",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e734625",
   "metadata": {},
   "source": [
    "Eigen decomposition, or eigendecomposition, has numerous real-world applications across various fields.\n",
    "\n",
    "1. PCA is a dimensionality reduction technique that uses eigendecomposition to identify the principal components (eigenvectors) of a covariance matrix. It is widely used in data analysis, pattern recognition, and image processing to reduce the dimensionality of datasets while preserving the most important information.\n",
    "\n",
    "2. In quantum mechanics, eigendecomposition is fundamental for solving problems related to observables, such as the position and momentum of particles. The eigenvectors of operators represent the states of definite properties, and the eigenvalues represent the possible values of those properties.\n",
    "\n",
    "3. Eigendecomposition is utilized in image compression techniques. By representing images in terms of their principal components (eigenvectors), it's possible to capture and store the most significant features while discarding less important details.\n",
    "\n",
    "4. Eigendecomposition is applied in control systems to analyze stability and control the behavior of dynamic systems. It is used to understand the modes of motion and response of robotic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56aae0c",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021d22bc",
   "metadata": {},
   "source": [
    "A square matrix can have multiple sets of eigenvectors and eigenvalues, but each set is associated with different eigenvectors and eigenvalues. In other words, for a given matrix, there can be distinct sets of eigenvectors and eigenvalues.\n",
    "\n",
    "If a matrix has distinct eigenvalues, each eigenvalue corresponds to a unique set of linearly independent eigenvectors. In this case, the sets of eigenvectors are different, and each set is associated with a specific eigenvalue.\n",
    "\n",
    "It's important to note that the number of linearly independent eigenvectors associated with a distinct eigenvalue is always less than or equal to the algebraic multiplicity of that eigenvalue (the number of times it appears as a root of the characteristic polynomial). The eigenvectors corresponding to distinct eigenvalues, however, are always linearly independent.\n",
    "\n",
    "In summary, while a matrix can have multiple sets of eigenvectors and eigenvalues, each set is associated with a unique eigenvalue, and the eigenvectors within a set are linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eedea5",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3f01c",
   "metadata": {},
   "source": [
    "Eigen-Decomposition is a powerful technique in data analysis and machine learning, providing valuable insights and solutions in various applications. Here are three specific ways in which the Eigen-Decomposition approach is utilized:\n",
    "\n",
    "1. Principal Component Analysis (PCA): PCA aims to find the principal components, which are the eigenvectors of the covariance matrix of a dataset. The eigenvalues associated with these eigenvectors indicate the amount of variance along each principal component. By selecting the top eigenvectors with the largest eigenvalues, one can reduce the dimensionality of the data while retaining most of the important information.\n",
    "\n",
    "2. Spectral Clustering: Spectral clustering involves creating a similarity or affinity matrix from the data and then finding its eigenvectors. The eigenvectors associated with the k smallest eigenvalues are used to embed the data into a lower-dimensional space, and clustering is performed in this space.\n",
    "\n",
    "3. PageRank Algorithm: The PageRank algorithm, developed by Google, models the importance of web pages in terms of their hyperlink structure. The algorithm represents the web as a matrix, and the dominant eigenvector of this matrix corresponds to the PageRank scores of web pages. Higher PageRank values indicate more influential pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c94bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
