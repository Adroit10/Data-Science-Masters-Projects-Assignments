{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7552cff",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1c3b73",
   "metadata": {},
   "source": [
    "The filter method is a technique used in feature selection, a process where relevant features are chosen to build a model while irrelevant or redundant features are excluded. The filter method evaluates the characteristics of individual features without considering the interaction between features. It relies on statistical measures to assess the importance of each feature and then selects or removes features based on those measures.\n",
    "\n",
    "The filter method evaluates the feature besed on metrics like correlation, information gain, Chi-squared, ANOVA. According to these scores we rank the features and select the best features out of all based on their score for training the model. \n",
    "\n",
    "Advantages of the filter method include simplicity, efficiency, and independence from specific machine learning algorithms. However, it may not capture the interactions between features, and the selected features are chosen without considering the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7a7344",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae4a33",
   "metadata": {},
   "source": [
    "Wrapper methods and filter methods are two different approaches to feature selection in machine learning, and they differ in their methodologies and how they incorporate the machine learning model during the selection process.\n",
    "\n",
    "\n",
    "Wrapper methods use a specific machine learning model to evaluate the performance of different subsets of features. The performance of the model is assessed based on metrics such as accuracy, precision, recall, F1 score, or other relevant criteria. Wrapper methods perform an exhaustive search or use heuristic techniques to evaluate different combinations of features. This involves training the model multiple times with different subsets of features to identify the optimal set that maximizes the model's performance.\n",
    "\n",
    "\n",
    "\n",
    "Wrapper methods are generally more computationally expensive due to multiple model training cycles. Filter methods are computationally more efficient as they do not involve training the model during the feature selection process. Wrapper methods are tied to a specific machine learning model, and the effectiveness of selected features depends on the chosen model. Wrapper methods perform an exhaustive or heuristic search over different feature subsets while filter methods evaluate features independently and select them based on predefined criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61387ed",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c91ca",
   "metadata": {},
   "source": [
    "Embedded feature selection techniques integrate the feature selection process into the model training itself. These methods automatically select the most relevant features during the learning process.\n",
    "\n",
    "Some common embedded feature selection methods are: \n",
    "1. LASSO : This regularization technique used in linear regression adds a penalty term to the cost function of the regression algorithm which constitutes of the sum of the absolute values of the coefficients multiplied by the regularization parameter lambda which  forces some coefficients to be exactly zero.\n",
    "\n",
    "2. Ridge regression: Similar to LASSO, ridge regression is a regularization technique that adds a penalty term to the linear regression objective function. It helps prevent overfitting by shrinking the coefficients, but it doesn't force them to be exactly zero.\n",
    "\n",
    "3. Elastic Net : Elastic Net is a combination of LASSO and Ridge Regression. It uses a linear combination of both L1 and L2 regularization terms. This allows for feature selection while handling the correlation between features.\n",
    "\n",
    "4. Decision Trees and ensemble techniques like Random Forest: Decision trees inherently perform feature selection by selecting the most important features at each split. Random Forests, which are an ensemble of decision trees, can be used to assess feature importance and select relevant features.\n",
    "\n",
    "5. Gradient Boosting : Gradient Boosting Machines e.g., XGBoost use an ensemble of weak learners to build a strong predictive model. These algorithms assign importance scores to features during training, and features with low importance can be pruned.\n",
    "\n",
    "6. Neural Networks with Dropout: In neural networks, dropout is a regularization technique where randomly selected neurons are ignored during training. This implicitly results in a form of feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cbb93f",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2332b52",
   "metadata": {},
   "source": [
    "While filter method has it's advantage for feature selection process it also has some disadvantages:\n",
    "\n",
    "1. Ignores Feature interactions: Filter methods evaluate features independently of each other. They do not consider the interactions or dependencies between features. In real-world datasets, features often interact, and important information might be captured through combinations of features.\n",
    "\n",
    "2. Does'nt consider model performance: Filter methods assess the relevance of features based on statistical measures but do not consider how well the selected features contribute to the performance of the final machine learning model. The selected features may not be the most effective for the specific modeling task.\n",
    "\n",
    "3. Sensitivity to Correlated Features: Filter methods may not handle well situations where features are highly correlated. Redundant features might be retained, or important features might be excluded if their importance is diluted by highly correlated features.\n",
    "\n",
    "4. May Not Perform Well with Noisy Data: Filter methods are sensitive to noisy or irrelevant features in the dataset. If the dataset contains a significant amount of noise, filter methods may struggle to identify the truly relevant features.\n",
    "\n",
    "5. May Not Capture Non-linear Relationships: Filter methods are often based on linear or monotonic relationships, which may not capture non-linear relationships between features and the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba38faf7",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ca630",
   "metadata": {},
   "source": [
    "The choice between the filter method and the wrapper method for feature selection depends on various factors, including the characteristics of the dataset, the computational resources available, and the goals of the machine learning task.\n",
    "\n",
    "1. Large Datasets:  Filter methods are computationally less expensive compared to wrapper methods. If you have a large dataset with a high number of features, filter methods may be more practical and efficient in terms of processing time.\n",
    "\n",
    "2. High dimensionality : In situations where the dataset has a high dimensionality (many features), filter methods can quickly reduce the number of features without the need for extensive model training. This can be beneficial in scenarios with a large number of potential predictors.\n",
    "\n",
    "3. Noise in the Dataset: If the dataset contains noisy features or irrelevant variables that are not well-correlated with the target variable, filter methods may be more robust. They can identify and filter out such features based on statistical measures.\n",
    "\n",
    "4. Preprocessing Step: Filter methods are often used as a preprocessing step before applying more computationally expensive wrapper or embedded methods. They can help in reducing the dimensionality of the dataset and simplifying the subsequent feature selection process.\n",
    "\n",
    "5. Stability Across Models: Filter methods are generally more stable across different models. If you are uncertain about the final modeling approach and want a feature selection method that is less dependent on the specific modeling algorithm, filter methods may be preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb714a18",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f063a362",
   "metadata": {},
   "source": [
    "Let's suppose we are working in a telecom company and hav eto develop a predictive model using a vast numbers of features provided to us. We can't use all the features for the purpose of our predictions as all features might not be relevant for our model. Thus for this purpose we might adopt filter method for selection of relevant features for our model. The filter method is a technique used in feature selection, a process where relevant features are chosen to build a model while irrelevant or redundant features are excluded. The filter method evaluates the characteristics of individual features without considering the interaction between features. It relies on statistical measures to assess the importance of each feature and then selects or removes features based on those measures.\n",
    "\n",
    "\n",
    "We calculate the correlation of different features with the dependant variable and rank the features according to the score. The most important  features will be ranked highly and thus can be used for the purpose of our model development. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e63c2",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb01e71",
   "metadata": {},
   "source": [
    "While working on a dataset to predict the outcome of a soccer match we might have a large number of features including player statistics and team rankings, thus it's important tol filter out the most important features as the less important features might not contribute effectively for our purpose of development of the model and just increase the complexity of our model. Embedded method of feature selection uses both the advantages of filter and wrapper methods.\n",
    "\n",
    "For executing this we can use methods like Lasso or ridge regression which helps in feature selection along with linear regression but for this the dataset must follow  a linear relationship. If the features don' thave a linear relationship with the output feature we can use algorithms like decision trees or ensemble techniques like Random Forrest which provides the feature importance metric to evaluate th eimportance of various features in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7bb5a1",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea8b53",
   "metadata": {},
   "source": [
    "While working on a housing dataset which is based of limited features like the size, location and age of the house we can make use of wrapper methods to select the most relevant features. In wrapper we use a specific machine learning model to evaluate the performance of different subsets of features. The performance of the model is assessed based on metrics such as accuracy, precision, recall, F1 score, or other relevant criteria. Wrapper methods perform an exhaustive search or use heuristic techniques to evaluate different combinations of features. This involves training the model multiple times with different subsets of features to identify the optimal set that maximizes the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88afa03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
