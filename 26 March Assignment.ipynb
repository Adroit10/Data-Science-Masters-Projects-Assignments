{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12a0e1c3",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1953f60",
   "metadata": {},
   "source": [
    "Linear Regression:\n",
    "It establshes a relationship between 2 variables ann independant variable and the output dependant variable. Graphically linear regression is depicted as a straight line with the slope defining the rate of change of dependant variable with the change in the independant variable.\n",
    "      \n",
    "      \n",
    "                    y = x0 + m*x1\n",
    "    \n",
    "      Where  x0: is the intercept\n",
    "             m: is the slope \n",
    "             x1: is the independant data point\n",
    "\n",
    "Example:\n",
    "when we predict the weight of an individual by knowing the height. \n",
    "\n",
    "\n",
    "Multiple Linear Regression:\n",
    "It establishes a relationship between an output dependant variable and multiple input independant variables. It assumes that there is a correlation between each of the independant variables and the single dependant variable.\n",
    "               \n",
    "               y = x0 + m1*xx1 + m2*x2 .... mn*xn\n",
    "      \n",
    "      Where     x0 : is the intercept \n",
    "                m1,m2,..mn: slopes at different data points\n",
    "                x1,x2,...xn: independant variables\n",
    "        \n",
    " Example:\n",
    " When we need to predict the price of a house based on the number of rooms, locality, age of the building..\n",
    "               \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ed7e4a",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399cf46e",
   "metadata": {},
   "source": [
    "Assumptions of simple linear regression:\n",
    "1. linearity: The relationship between X and Y must be linear.\n",
    "2. Independanece of errors: Y must be independant of errors.\n",
    "3. Normality of errors: The residuals must be approximately normally distributed\n",
    "4. Equal Variance: The variance of the residuals is the same for all values of X\n",
    "\n",
    "\n",
    "Testing the assumptions:\n",
    "1. To check linearity of the data points we draw the scatterplot of the given data points\n",
    "2. we can detect the independance of residual errors by the plot of residual erros versus the predicted values or the plot of residual errors vs actual values\n",
    "3. To check the normality of the residual errors we can check the skewness adn kurtosis of the distribution of residual errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f1b9f6",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e23fd3",
   "metadata": {},
   "source": [
    "The slope is the steepness of the line and the intercept is the point where the line cuts the axis, they both define the linear relationship between the two variables.For example a company determines the job performance of employess by using the regression model where x is the number of hours of work and y is the score on job skill and the intercept is the average job skill score of employee without any training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe4b49f",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe703ae0",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization algorithm used to find the local minimum /maximum of a function, in machine learning it is used to minimize the cost function of a model by iteratively adjusting its parameters in the opposite direction of the gradient. The gradient is the slope of the cost function, and by moving in the direction of the negative gradient, the algorithm can converge to the optimal set of parameters that best fits the training data. It can be applied to many machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bdf8db",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52956e9",
   "metadata": {},
   "source": [
    "Multiple Linear Regression:\n",
    "It establishes a relationship between an output dependant variable and multiple input independant variables. It assumes that there is a correlation between each of the independant variables and the single dependant variable.\n",
    "               \n",
    "               y = x0 + m1*xx1 + m2*x2 .... mn*xn\n",
    "      \n",
    "      Where     x0 : is the intercept \n",
    "                m1,m2,..mn: slopes at different data points\n",
    "                x1,x2,...xn: independant variables\n",
    "        \n",
    " Example:\n",
    " When we need to predict the price of a house based on the number of rooms, locality, age of the building..\n",
    "               \n",
    "It is different from linear regression as it has more than one independant variable with which the dependant variable is correlated\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad1db1",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80085398",
   "metadata": {},
   "source": [
    "Multicollinearity occurs when two or more independent variables in a data frame have a high correlation with one another in a regression model. Multicollinearity can be a problem in a regression model when using algorithms such as OLS (ordinary least squares) in statsmodels. This is because the estimated regression coefficients become unstable and difficult to interpret in the presence of multicollinearity.When multicollinearity is present, the estimated regression coefficients may become large and unpredictable, leading to unreliable inferences about the effects of the predictor variables on the response variable.\n",
    "\n",
    "multicollinearity is detected by using Variance Inflation Factor(VIF), Lasso regression, Ridge regression and elastic net regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45699c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b95d69d",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680fb557",
   "metadata": {},
   "source": [
    "Polynomial regression describes the non linear relationship between the independant and the dependant variables. \n",
    "               \n",
    "               y = x0 + b1*x1 + b2*x^2...\n",
    "     Where  x0: is the intercept\n",
    "             m: is the slope \n",
    "             x1: is the independant data point\n",
    "Polynomial regression is a form of Linear regression where only due to the Non-linear relationship between dependent and independent variables, we add some polynomial terms to linear regression to convert it into Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ed6ac7",
   "metadata": {},
   "source": [
    "# Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b49606f",
   "metadata": {},
   "source": [
    "Advantages of Polynomial Regression:\n",
    "1. Works on any size of dataset\n",
    "2. works well for non linear problems\n",
    "\n",
    "Disadvantages of Polynomial regression:\n",
    "1. We need to choose the right polynomial degree for good bias tradeoff not overfitting the datapoints.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6842a25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
