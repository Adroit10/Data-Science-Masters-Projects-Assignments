{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "292b8573",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994e1de3",
   "metadata": {},
   "source": [
    "KNN is a non-parametric, instance-based learning algorithm, meaning it doesn't make any assumptions about the underlying data distribution. It can be effective in certain scenarios, especially when the decision boundaries are irregular or when the data is not linearly separable. It is a simple and widely used classification and regression algorithm in machine learning. In short KNN does not build a model explicitly but instead memorizes the training instances. During the training phase, the algorithm simply stores the training dataset.\n",
    "To make a prediction for a new, unseen data point, the algorithm identifies the 'k' nearest neighbors in the training dataset based on some distance metric. The class label of the majority of these 'k' neighbors is assigned to the new data point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152704dd",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79f5857",
   "metadata": {},
   "source": [
    "Choosing the right value of 'k' in the K-Nearest Neighbors (KNN) algorithm is crucial, as it can significantly impact the performance of the model. The selection of 'k' depends on the characteristics of the dataset, and there isn't a one-size-fits-all answer. \n",
    "\n",
    "1. When dealing with binary classification problems, it's often recommended to choose an odd value for 'k.' This helps avoid ties when voting for the majority class, ensuring a clear decision.\n",
    "\n",
    "2. One rule of thumb is to set 'k' to the square root of the number of instances in the dataset. This can be a good starting point for exploration.\n",
    "\n",
    "3. Use cross-validation to evaluate the performance of the model for different values of 'k.' This involves splitting the dataset into training and validation sets multiple times and assessing the model's performance. Choose the 'k' that provides the best balance between bias and variance.\n",
    "\n",
    "4. Perform a grid search over a range of 'k' values and choose the one that results in the best performance. This can be combined with cross-validation for a more robust evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a444b80",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a7e431",
   "metadata": {},
   "source": [
    "KNN is commonly used as a classification algorithm. It assigns a class label to a data point based on the majority class among its k-nearest neighbors. The output of a KNN classifier is a discrete class label indicating the category to which the input data point belongs.\n",
    "\n",
    "KNN can also be used for regression tasks. Instead of predicting a class label, a KNN regressor predicts a continuous value based on the average (or another measure) of the target values among its k-nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d3fd68",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5006c5be",
   "metadata": {},
   "source": [
    "The performance of a K-Nearest Neighbors (KNN) algorithm can be evaluated using various metrics, depending on whether you are working on a classification or regression task. \n",
    "\n",
    "Classification Metrics:\n",
    "\n",
    "1. Accuracy: Accuracy provides an overall measure of the correct classifications. However, it may not be suitable for imbalanced datasets.\n",
    "\n",
    "2. Precision: Precision measures the accuracy of the positive predictions. It is relevant when the cost of false positives is high.\n",
    "\n",
    "3. Recall: Recall measures the ability of the classifier to identify all relevant instances. It is relevant when the cost of false negatives is high.\n",
    "\n",
    "4. F1 score: The F1 score is the harmonic mean of precision and recall. It provides a balance between the two metrics.\n",
    "\n",
    "\n",
    "Regression Metrics :\n",
    "\n",
    "1. Mean Squared error: MSE measures the average squared difference between the actual and predicted values. \n",
    "\n",
    "2. Mean Absolute Error : MAE measures the average absolute difference between the actual and predicted values.\n",
    "\n",
    "3. R-Squared : R-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, with higher values indicating better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5317b34d",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1925b922",
   "metadata": {},
   "source": [
    "The \"curse of dimensionality\" refers to the challenges and issues that arise when working with high-dimensional data in machine learning and data analysis. \n",
    "\n",
    "1. As the number of dimensions (features) increases, the available data becomes more sparse. In a high-dimensional space, data points are spread out, and the distance between them tends to increase. This sparsity can lead to difficulties in defining meaningful distances between data points. Calculating distances between points becomes computationally more expensive in high-dimensional spaces. The time complexity of the KNN algorithm is influenced by the number of dimensions, as each distance calculation involves considering all features.\n",
    "\n",
    "2. In high-dimensional spaces, the concept of proximity or similarity becomes less meaningful. Points that are close together in high-dimensional space may not be close in terms of meaningful relationships. This can impact the ability of KNN to accurately identify neighbors and make reliable predictions.\n",
    "\n",
    "3. With an increasing number of dimensions, the risk of overfitting also grows. KNN may become more sensitive to noise in the data, capturing spurious patterns that do not generalize well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52ea23",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaef8c68",
   "metadata": {},
   "source": [
    "Handling missing values in a dataset is crucial for the proper functioning of machine learning algorithms, including K-Nearest Neighbors (KNN). \n",
    "\n",
    "1. Imputation: \n",
    "We can use KNN Inputation to impute missing values. For each missing value, find the k-nearest neighbors based on the available features and use their values to impute the missing value. This method can be more effective when the data has a complex structure.\n",
    "\n",
    "2. Deletion: \n",
    "Remove entire rows containing missing values. This is suitable when the number of missing values is relatively small and doesn't significantly impact the dataset. Or we can remove features (columns) with a high percentage of missing values. This is appropriate when the missing values are concentrated in a few features, and those features are not critical for the analysis.\n",
    "\n",
    "3. Train machine learning models (including KNN) to predict missing values based on other features. This can be more accurate when relationships between variables are complex.\n",
    "\n",
    "4. Consider domain-specific knowledge to handle missing values. For example, if certain values are missing systematically, it might indicate a specific pattern or reason that can be addressed accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ef07f9",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3778af49",
   "metadata": {},
   "source": [
    "The choice between using a K-Nearest Neighbors (KNN) classifier or regressor depends on the nature of the problem and the type of output variable you are trying to predict.\n",
    "\n",
    "KNN Classifier: \n",
    "Classifies data points into discrete classes or categories. It is suitable for tasks when the target variable is categorical and the goal is to assign each data point to one of several predefined classes. Metrics such as accuracy, precision, recall are used for evaluation purposes.\n",
    "\n",
    "KNN Regressor:\n",
    "Predicts continuous values for the target variable. It provides a numerical value as the prediction. It is used when the target variable is continuous and the goal is to predict a specific numerical value. KNN regressor tends to be less sensitive to outliers than the KNN classifier. Metrics such as Mean Squared error , Mean Absolute Error are used to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c799e",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a1ee7",
   "metadata": {},
   "source": [
    "Strengths of KNN :\n",
    "\n",
    "1. Simple and intuitive: KNN is straightforward to understand and implement. It is based on the intuitive concept of proximity.\n",
    "\n",
    "2. KNN is a non-parametric algorithm, meaning it does not make assumptions about the underlying data distribution. It can adapt to complex decision boundaries.\n",
    "\n",
    "3. KNN can handle non-linear and irregular decision boundaries, making it suitable for datasets with complex structures.\n",
    "\n",
    "4. KNN can be effective when working with small to medium-sized datasets where the computational cost is manageable.\n",
    "\n",
    "5. KNN does not involve a separate training phase; it memorizes the entire dataset during the training process.\n",
    "\n",
    "\n",
    "Weaknesses of KNN :\n",
    "\n",
    "1. Calculating distances between data points can be computationally expensive, especially in high-dimensional spaces. This affects the algorithm's efficiency as the dataset grows.\n",
    "\n",
    "2. KNN is sensitive to noisy data and outliers, as they can significantly impact the calculation of distances and influence predictions.\n",
    "\n",
    "3. The performance of KNN can degrade in high-dimensional spaces due to the curse of dimensionality. As the number of features increases, the distance between data points becomes less meaningful.\n",
    "\n",
    "4. KNN is sensitive to the scale of features. It is recommended to scale features to ensure that all dimensions contribute equally to distance calculations.\n",
    "\n",
    "\n",
    "\n",
    "To mitigate these weakness we can :\n",
    "\n",
    "1. Use dimensionality reduction techniques e.g., PCA to reduce the number of features and mitigate the curse of dimensionality.\n",
    "\n",
    "2. Normalize or standardize features to ensure that all dimensions contribute equally to distance calculations.\n",
    "\n",
    "3. Identify and handle outliers using techniques such as outlier detection algorithms or robust normalization methods.\n",
    "\n",
    "4. Perform cross-validation to find an optimal value for 'k' that balances bias and variance. Avoid very small or very large values of 'k.'\n",
    "\n",
    "5. Experiment with different distance metrics based on the characteristics of the data. Common choices include Euclidean distance, Manhattan distance, or other domain-specific metrics.\n",
    "\n",
    "6. Implement parallelization techniques to speed up distance calculations and improve computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d39ae72",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817f9073",
   "metadata": {},
   "source": [
    "Euclidean distance and Manhattan distance are two different distance metrics used in the context of the K-Nearest Neighbors (KNN) algorithm. These metrics quantify the distance between two points in a multidimensional space, and the choice between them can impact the behavior of the KNN algorithm. \n",
    "\n",
    "1. Euclidean Distance: \n",
    "\n",
    "In Euclidean distance, the distance between two points is calculated as the straight-line distance between them, commonly known as the \"as-the-crow-flies\" distance. It is the geometric distance between two points in a Euclidean space. Euclidean distance tends to be sensitive to differences in all dimensions, and it increases as the number of dimensions increases (curse of dimensionality). Euclidean distance is often preferred when the relationships between features are more continuous, and a straight-line distance is more representative of similarity.\n",
    "\n",
    "2. Manhattan Distance:\n",
    "\n",
    "Manhattan distance, also known as L1 norm or Taxicab distance, measures the distance between two points by summing the absolute differences between their coordinates. It represents the distance a taxi would travel in a grid-like city (where movement can only occur along the grid lines) to reach the destination.\n",
    "Manhattan distance is less sensitive to changes in dimensions compared to Euclidean distance. Manhattan distance may be more suitable when features have a more step-like or grid-like relationship, and the direct path is less meaningful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13234b1f",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f49a8",
   "metadata": {},
   "source": [
    "Feature scaling plays a crucial role in the performance of the K-Nearest Neighbors (KNN) algorithm. KNN relies on the calculation of distances between data points, and the scale of features can significantly impact these distance measurements. KNN is sensitive to the scale of features. Features with larger magnitudes can dominate the distance calculations. If one feature has a range of values much larger than another, the contribution of the smaller-range feature may be negligible.\n",
    "\n",
    "Feature scaling can lead to improved model performance by ensuring that all features contribute equally to distance calculations. It can help algorithms like KNN converge more quickly, especially when using distance-based metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b5c2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
