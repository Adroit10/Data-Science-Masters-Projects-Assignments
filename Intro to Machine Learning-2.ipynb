{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a89b60",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ad3a3f",
   "metadata": {},
   "source": [
    "#### Overfitting\n",
    " Overfitting occurs when a model learns the training data too well, capturing noise and random fluctuations rather than the underlying patterns. As a result, the model performs poorly on new, unseen data.\n",
    " \n",
    "The consequences of overfitting are :\n",
    "1. High training accuracy but poor generalization to new data.\n",
    "2. Sensitivity to noise and outliers in the training data.\n",
    "3. Overly complex models that may not generalize well.\n",
    "\n",
    "\n",
    "Mitigation::\n",
    "1. Regularization: introducing penalty term to the cost function of model discourages overfitting.\n",
    "2. Cross-validation: techniques like k-fold cross-validation to assess model performance on multiple subsets of the data.\n",
    "3. Feature Selection: Select only the most relevant features to reduce model complexity.\n",
    "4. Early Stopping: Monitor performance on a validation set during training and stop when performance starts to degrade.\n",
    "\n",
    "\n",
    "#### Underfitting\n",
    "\n",
    "Underfitting occurs when a model is too simple to capture the underlying patterns in the training data. The model may fail to learn the relationships between features and target outputs, leading to poor performance on both training and new data.\n",
    "\n",
    "Consequences::\n",
    "1. Low training accuracy and poor generalization to new data.\n",
    "2. Failure to capture important patterns in the data.\n",
    "3. Inability to make accurate predictions.\n",
    "\n",
    "Mitigation ::\n",
    "\n",
    "1.  Use a more complex model with more parameters.\n",
    "2. Add more relevant features to the model.\n",
    "3.  If regularization is too high, it may lead to underfitting. Adjust regularization parameters accordingly.\n",
    "4. If the current model is too simple, consider using a more complex one.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc5591b",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4b58ca",
   "metadata": {},
   "source": [
    "Overfitting occurs when a model learns the training data too well, capturing noise and random fluctuations rather than the underlying patterns. As a result, the model performs poorly on new, unseen data. Therefore to reduce overfitting we use techniques like regularization e.g, ridge and lasso regularizers. We can perform cross validation techniques like k-fold cross validation where subsets of the data are used to train and assess the model's accuracy. Feature selection is also an important and helpful step to overcome overfitting as it reduces the complexity in the data and thus our model does'nt become too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866a4ffb",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b825e",
   "metadata": {},
   "source": [
    "Underfitting occurs when a model is too simple to capture the underlying patterns in the training data. The model may fail to learn the relationships between features and target outputs, leading to poor performance on both training and new data. There are several scenarios that lead upto underfitting :\n",
    "\n",
    "1. Insufficient model complexity can lead to underfitting. For example using linear models to solve problems with complex non linear relationships.\n",
    "\n",
    "\n",
    "2. If our model has too few features then alson it leads to underfitting Because our model may fail to capture the important relationships to the dependant variable thus giving us poor generalization.\n",
    "\n",
    "3. If we use excessive regularizations like L1 and L2 the model can become too simplistic as it penalizes the the model for being too complex.\n",
    "\n",
    "4. Termination of the training process too early using less number of epochs may also lead to underfitting. \n",
    "\n",
    "5. Failing to scale the data properly especially for algorithms which require feature scaling can also lead to sub-optimal performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcfce0a",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44044eec",
   "metadata": {},
   "source": [
    "The bias-variance trade-off is a fundamental concept in machine learning that describes relationship between bias, variance and model complexity.\n",
    "\n",
    "#### Bias:\n",
    "\n",
    "Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias can lead to the model consistently underpredicting or overpredicting the true values. High bias models are often too simplistic and fail to capture the underlying patterns in the data. Models with high bias tend to have poor performance on both the training and test sets.\n",
    "\n",
    "\n",
    "#### Variance:\n",
    "\n",
    "Variance measures the model's sensitivity to small fluctuations in the training data. High variance can lead to the model being too flexible and capturing noise in the training data. High variance models are often overly complex, fitting the training data closely but failing to generalize well to new, unseen data.\n",
    "\n",
    "The bias-variance trade-off refers to the delicate balance between bias and variance. As you decrease bias (make the model more complex), variance tends to increase, and vice versa.The goal is to find the optimal level of model complexity that minimizes both bias and variance, leading to the best generalization to new, unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a4774",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fcbe6b",
   "metadata": {},
   "source": [
    "Detecting overfitting and underfitting in machine learning models is crucial for building models that generalize well to new, unseen data.\n",
    "\n",
    "1. Learning curves:\n",
    "\n",
    "Plot learning curves that show the model's performance on the training and validation sets over time. Overfitting is indicated by a large gap between training and validation performance.High training accuracy but poor test accuracy.\n",
    "\n",
    "For underfitted model learning curves show poor performance on both training and validation sets, suggesting the model is too simple. Low training and validation accuracy.\n",
    "\n",
    "2. Performance Metrics: \n",
    "\n",
    "Compare training and test set performance metrics, such as accuracy, precision, recall, or F1 score. Overfitting is indicated by a significant drop in performance on the test set. \n",
    "\n",
    "Consistently low performance metrics on both training and test sets suggest underfitting.\n",
    "\n",
    "3. Validation Set Performance:\n",
    "\n",
    "Monitor performance on a separate validation set during training. A significant drop in validation set performance may indicate overfitting.\n",
    "\n",
    "Poor performance on the training set may suggest underfitting.\n",
    "\n",
    "4. Cross-Validation:\n",
    "\n",
    "Use techniques like k-fold cross-validation to assess model performance on multiple subsets of the data. If performance varies widely across folds, it may indicate overfitting.\n",
    "\n",
    "If Cross-validation consistently shows poor performance, suggesting that the model is not capturing the underlying patterns in the data.\n",
    "\n",
    "5. Feature Importance Analysis:\n",
    "\n",
    " Analyze feature importance to identify if the model is giving too much weight to specific features. High feature importance for irrelevant features may indicate overfitting.\n",
    " \n",
    " If the features are not contributing significantly to the model, suggesting a too simplistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf3ff3",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee2cbc7",
   "metadata": {},
   "source": [
    "Comparison between bias and variance:\n",
    "\n",
    "1. Performance on Training Data:\n",
    "\n",
    "High bias models tend to have low accuracy on the training data because they are too simplistic.\n",
    "\n",
    "High variance models can have high accuracy on the training data, fitting it closely.\n",
    "\n",
    "2. Performance on Test Data (Generalization):\n",
    "\n",
    "High bias models may generalize poorly to new, unseen data, leading to systematic errors.\n",
    "\n",
    " High variance models often generalize poorly to new data, leading to erratic errors.\n",
    "\n",
    "3. Sensitivity to Noise:\n",
    "\n",
    "Bias is less sensitive to noise, as the model is too simple to be influenced significantly by individual data points.\n",
    "\n",
    "Variance has High sensitivity to noise, as the model may fit the noise in the training data.\n",
    "\n",
    "\n",
    "Example for high bias model: Using a linear regression model to predict the price of houses based on a highly nonlinear relationship between features and prices.\n",
    "\n",
    "\n",
    "Example for high variance model: Using a high-degree polynomial regression model to predict house prices when a simpler model would be sufficient.\n",
    "\n",
    "\n",
    "Different trade-offs between bias and variance:\n",
    "\n",
    "1. High Bias, Low Variance:\n",
    "\n",
    "These models may oversimplify the data, leading to systematic errors (underfitting). They perform poorly on both training and test sets. \n",
    "\n",
    "2. Low Bias, High Variance:\n",
    "\n",
    "These models may fit the training data very closely but fail to generalize, performing well on the training set but poorly on the test set (overfitting).\n",
    "\n",
    "3. Low Bias, Low Variance:\n",
    "\n",
    "These models strike a balance, capturing essential patterns in the data without being overly sensitive to noise. They tend to perform well on both training and test sets. Thus leading to a generalized model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be6775f",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5266f4cc",
   "metadata": {},
   "source": [
    "Regularization in machine learning is a technique used to prevent overfitting by adding a penalty term to the objective function. The primary purpose of regularization is to encourage the model to be less complex, preventing it from fitting the training data too closely and improving its generalization to new, unseen data. Regularization is commonly applied to linear regression, logistic regression, and neural networks.\n",
    "\n",
    "There are two main types of regularization: L1 regularization (Lasso) and L2 regularization (Ridge).\n",
    "\n",
    "1. L1 Regularization (Lasso):\n",
    "\n",
    "The regularization term is the absolute sum of the model coefficients multiplied by a regularization parameter lambda.  L1 regularization tends to produce sparse models by driving some coefficients to exactly zero, effectively performing feature selection. This regularization technique is used when there is a belief that many features are irrelevant or when feature selection is crucial.\n",
    "\n",
    "2. L2 Regularization (Ridge) : \n",
    "\n",
    "The regularization term is the sum of the squared values of the model coefficients multiplied by a regularization parameter lambda. L2 regularization encourages the model to distribute the weight more evenly across all features, preventing any single feature from dominating. This regularization is used when all features are expected to contribute to the model, and no feature selection is required.\n",
    "\n",
    "\n",
    "\n",
    "There is also another type of regularization known as the Elastic Net which i sthe combination of both  Ridge and Lasso Regularization. It balances the benefits of both Ridge and Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeeb0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
