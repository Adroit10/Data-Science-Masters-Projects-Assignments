{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "210c2627",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f54040",
   "metadata": {},
   "source": [
    "Lasso regression also known as L1 regularization adds up a penalty term to the ordinary least square cost function (used for linear regression). The penalty term consisists of the absolute value of the coefficients multiplied by a regularization parameter (lambda). The goal of lasso is to minimise the cost function while keeping the absolute values small. Lasso regression helps in feature selection. It minimizes the affect of the feature that is not much correlated to the output variable.\n",
    "\n",
    "As we keep changing the lambda value the value of the global minima keeps decreasing and the value of theta or coefficient becomes zero eventually thus that particular feature gets dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d368e",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74344e91",
   "metadata": {},
   "source": [
    "The main advantage of using Lasso regression for feature selection is its ability to induce sparsity in the model, meaning it can force some of the coefficients of the features to be exactly zero. Lasso regression can automatically select a subset of the most important features by setting the coefficients of less relevant features to zero. This is particularly useful in situations where there are a large number of features, and it may be challenging to manually identify the most important ones.\n",
    "Lasso can provide guidance on which features are more important for the predictive task, helping in the process of feature engineering and feature understanding.\n",
    "\n",
    "By reducing the number of features, Lasso can help improve the generalization performance of the model, especially when the number of features is large compared to the number of observations. This can lead to models that are less prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a8347",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b9899",
   "metadata": {},
   "source": [
    "Interpreting the coefficients in Lasso is simmilar to that in linear regression model but with an added constraint which can become zero due to sparsity inducing nature of Lasso.\n",
    "If the coeficient for a paarticular feature is non zero it means that the feature has a non-negligible impact on the predicted outcome. The sign of the coefficient sgnifies the directio of the relationship. If the coefficient becomes zero it means that it can be removed from our model and has negligible effect on the output variable. The value of the coefficients aso signifies the importance of tghe features, the more the value of the coefificients the more is the importance of that partivcular fetaure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1610fd00",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c07708e",
   "metadata": {},
   "source": [
    "In Lasso regression the primary tuning parameter is the regularization parameter, often denoted as lambda.This parameter controls the strength of the penalty applied to the absolute values of the coefficients in the model. The Lasso objective function is a combination of the traditional least squares loss and the regularization term, and adjusting lambda impacts the trade-off between fitting the data well and keeping the model simple. A larger lambda value encourages sparsity in the model.  This can lead to more coefficients being exactly zero, effectively performing feature selection. However, an excessively high lambda may result in underfitting, where the model is too simple and unable to capture the underlying patterns in the data.\n",
    "\n",
    "A smaller lambda  reduces the penalty for non-zero coefficients, allowing more features to have non-zero coefficients. This may lead to a more complex model that fits the training data well but could be prone to overfitting, especially if the number of features is large compared to the number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf5e8fa",
   "metadata": {},
   "source": [
    "## Question 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36303e27",
   "metadata": {},
   "source": [
    "Lasso Regression is fundamentally a linear regression technique. It is designed for problems where the relationship between the input features and the target variable is assumed to be linear. In a non-linear regression problem, the relationship between the features and the target variable is not linear, and a linear model may not capture the underlying patterns in the data. But you can extend the idea of Lasso to polynomial regression by adding regularization terms to the coefficients of the polynomial features. This is known as regularized polynomial regression or regularized regression with basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342efd1b",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468308d",
   "metadata": {},
   "source": [
    "Ridge regression is also known as L2 regularization. The regularization term added to the ordinary least squares (OLS) objective function is the sum of the squared values of the coefficients, multiplied by a regularization parameter lambda. This term penalizes the square of the coefficients, encouraging smaller but non-zero coefficients. Ridge regression tends to shrink the coefficients towards zero but rarely sets them exactly to zero therefore it does not perfoem feature selection. It is effective in dealing with multicollinearity by distributing the impact of correlated features more evenly.\n",
    "\n",
    "The regularization term added is the sum of the absolute values of the coefficients, multiplied by a regularization parameter lambda. This term penalizes the absolute values of the coefficients and can lead to some coefficients being exactly zero, effectively performing feature selection. Lasso regression is known for its ability to induce sparsity in the model. It can set some coefficients exactly to zero, effectively performing feature selection. Lasso is particularly useful when there are many irrelevant or redundant features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c17d90",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566f4d4f",
   "metadata": {},
   "source": [
    "The regularization term in Lasso encourages sparsity in the model by penalizing the absolute values of the coefficients. This penalty has the effect of driving some coefficients to exactly zero, effectively performing automatic feature selection. In the presence of multicollinearity, Lasso can identify and prioritize features based on their contribution to the model's performance. This can be particularly useful in situations where features are redundant, as Lasso tends to select one representative from a group of correlated features. The effectiveness in handling multicollinearity is influenced by the choice of the regularization parameter larger lambda  increases the penalty for non-zero coefficients, leading to a sparser solution. The optimal parameter is chosen through techniques like cross-validaton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064dfa00",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea888e",
   "metadata": {},
   "source": [
    "The optimal value for the regularization paramaeter lambda can be chosen through techniques like cross validation to assess the generalization performance of the Lasso model. This helps ensure that the selected model is not overfitting or underfitting the training data and provides a more reliable interpretation of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82962f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e13c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
