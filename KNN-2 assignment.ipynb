{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbdbffe9",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33536370",
   "metadata": {},
   "source": [
    "Euclidean distance and Manhattan distance are two different distance metrics used in the context of the K-Nearest Neighbors (KNN) algorithm. These metrics quantify the distance between two points in a multidimensional space, and the choice between them can impact the behavior of the KNN algorithm.\n",
    "\n",
    "Euclidean Distance ::\n",
    "\n",
    "In Euclidean distance, the distance between two points is calculated as the straight-line distance between them, commonly known as the \"as-the-crow-flies\" distance. It is the geometric distance between two points in a Euclidean space. The formula involves taking the square root of the sum of squared differences between corresponding coordinates.\n",
    "Euclidean distance tends to be sensitive to differences in all dimensions, and it increases as the number of dimensions increases (curse of dimensionality).\n",
    "Euclidean distance is often preferred when the relationships between features are more continuous, and a straight-line distance is more representative of similarity.\n",
    "\n",
    "Manhattan Distance ::\n",
    "\n",
    "Manhattan distance, also known as L1 norm or Taxicab distance, measures the distance between two points by summing the absolute differences between their coordinates. It represents the distance a taxi would travel in a grid-like city (where movement can only occur along the grid lines) to reach the destination. The formula involves summing the absolute differences along each dimension. Manhattan distance is less sensitive to changes in dimensions compared to Euclidean distance.\n",
    "\n",
    "Manhattan distance may be more suitable when features have a more step-like or grid-like relationship, and the direct path is less meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1182b0e6",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce10388",
   "metadata": {},
   "source": [
    "Choosing the right value of 'k' in the K-Nearest Neighbors (KNN) algorithm is crucial, as it can significantly impact the performance of the model. The selection of 'k' depends on the characteristics of the dataset, and there isn't a one-size-fits-all answer.\n",
    "\n",
    "When dealing with binary classification problems, it's often recommended to choose an odd value for 'k.' This helps avoid ties when voting for the majority class, ensuring a clear decision.\n",
    "\n",
    "One rule of thumb is to set 'k' to the square root of the number of instances in the dataset. This can be a good starting point for exploration.\n",
    "\n",
    "Use cross-validation to evaluate the performance of the model for different values of 'k.' This involves splitting the dataset into training and validation sets multiple times and assessing the model's performance. Choose the 'k' that provides the best balance between bias and variance.\n",
    "\n",
    "Perform a grid search over a range of 'k' values and choose the one that results in the best performance. This can be combined with cross-validation for a more robust evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30701c1c",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df13fa6",
   "metadata": {},
   "source": [
    "The choice of distance metric in a K-Nearest Neighbors (KNN) classifier or regressor significantly affects the performance of the algorithm. Different distance metrics measure the similarity or dissimilarity between data points in distinct ways, and selecting an appropriate metric depends on the characteristics of the data and the nature of the problem.\n",
    "\n",
    "\n",
    "Consider the nature of your features and how you define similarity or dissimilarity. For example, Euclidean distance assumes continuous relationships, while Manhattan distance is more suitable for step-like relationships.\n",
    "\n",
    "If features have different scales, consider using distance metrics less sensitive to magnitude differences, such as Manhattan distance or cosine similarity.\n",
    "\n",
    "Consider the characteristics of your data. For example, if your data has outliers, robust distance metrics like Manhattan distance or Chebyshev distance may be appropriate.\n",
    "\n",
    "Domain knowledge and understanding of the problem can guide the choice of distance metric. For instance, if directionality is crucial, cosine similarity may be a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76330090",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02090aa",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) classifiers and regressors have hyperparameters that can significantly influence their performance. Tuning these hyperparameters is crucial to achieve optimal results.\n",
    "\n",
    "Common hyperparameters in KNN :\n",
    "\n",
    "1. Number of neighbours (k) : Specifies the number of nearest neighbors to consider when making predictions. In sklearn it si given as n_neighbours and the default value is equal to 5. We use cross-validation to find the optimal k value that balances bias and variance.\n",
    "\n",
    "2. Distance metric (metric in sklearn): Metric to use for distance computation. Default is “minkowski”, which results in the standard Euclidean distance when p = 2. \n",
    "\n",
    "3. Algorithm (auto, ball_tree, kd_tree, brute):  Determines the algorithm used for finding nearest neighbors. The choice of algorithm can impact the speed of neighbor searches, especially in high-dimensional spaces.\n",
    "\n",
    "4. Leaf Size (default size 30) : Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.\n",
    "\n",
    "5. Weighting Scheme:  Weight function used in prediction. Possible values:‘uniform’ : uniform weights. All points in each neighborhood are weighted equally. ‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.\n",
    "\n",
    "6. p (default 2) : Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0197ac3b",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed05fb10",
   "metadata": {},
   "source": [
    "The size of the training set can significantly impact the performance of a K-Nearest Neighbors (KNN) classifier or regressor. The amount of training data affects the model's ability to generalize to new, unseen instances.\n",
    "With a small training set:\n",
    "The model may overfit to noise in the data, capturing patterns that do not generalize well.\n",
    "The risk of high variance increases, and the model may be sensitive to the specific instances in the training set.\n",
    "\n",
    "With a large training set:\n",
    "The model is more likely to capture meaningful patterns in the data and generalize well to new instances.\n",
    "The risk of overfitting decreases, and the model becomes more robust.\n",
    "\n",
    "\n",
    "Techniques to Optimize Training Set Size:\n",
    "\n",
    "1. cross-validation : Use cross-validation to assess the model's performance across different subsets of the data. This helps evaluate how well the model generalizes and provides insights into potential overfitting.\n",
    "\n",
    "2. Learning Curves : Plot learning curves to visualize the relationship between the size of the training set and model performance. This helps identify the point of diminishing returns and whether more data is likely to be beneficial.\n",
    "\n",
    "3. Implement incremental or online learning strategies, where the model is updated as new data becomes available. This is useful when obtaining a large labeled dataset at once is challenging.\n",
    "\n",
    "4. Implement active learning strategies where the model actively selects instances for labeling to improve its performance. This is particularly useful when labeling large datasets is resource-intensive.\n",
    "\n",
    "5. Augment the training set by creating additional synthetic instances through techniques like rotation, scaling, cropping (for image data), or perturbing feature values. This is particularly useful when working with small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5202272",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aafe877",
   "metadata": {},
   "source": [
    "While K-Nearest Neighbors (KNN) is a simple and intuitive algorithm, it comes with some potential drawbacks that may affect its performance in certain scenarios. Here are some common drawbacks of using KNN as a classifier or regressor and strategies to overcome these limitations:\n",
    "\n",
    "1.  Computational Complexity:  Calculating distances between data points can be computationally expensive, especially in high-dimensional spaces or with large datasets.  \n",
    "Use data structures like KD-trees or Ball trees to speed up the search for nearest neighbors.\n",
    "Consider dimensionality reduction techniques to reduce the number of features.\n",
    "\n",
    "2. Sensitivity to Outliers and Noisy Data: KNN can be sensitive to outliers and noisy data points, which may significantly influence the majority voting or averaging process.\n",
    "Implement robust normalization techniques or use distance metrics less sensitive to outliers (e.g., Manhattan distance).\n",
    "Outlier detection and removal or robust weighting schemes can be applied.\n",
    "\n",
    "3. Categorical Features: KNN may struggle with datasets that contain categorical features or require specialized handling for categorical variables. \n",
    "Encode categorical features appropriately (e.g., one-hot encoding) to make them compatible with distance-based computations.\n",
    "Consider using distance metrics designed for categorical data.\n",
    "\n",
    "4. Imbalanced Data: KNN may struggle with imbalanced datasets, as the majority class can dominate predictions.\n",
    "Use techniques like oversampling or undersampling to balance the class distribution.\n",
    "Adjust class weights or use different evaluation metrics to account for imbalances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae9b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
